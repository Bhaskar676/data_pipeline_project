#!/bin/bash

echo "ğŸš€ Starting News Pipeline Data Engineering Assessment"
echo "=================================================="

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker first."
    exit 1
fi

# Set Airflow UID for Linux/macOS
export AIRFLOW_UID=50000

echo "ğŸ“‹ Pre-flight checks..."
echo "âœ… Docker is running"
echo "âœ… Environment variables set"

# Create necessary directories
mkdir -p logs plugins

echo "ğŸ—ï¸  Building and starting containers..."
docker-compose up -d

echo "â³ Waiting for services to start..."
sleep 30

echo "ğŸ” Checking service status..."
docker-compose ps

echo ""
echo "ğŸ‰ Pipeline is starting up!"
echo ""
echo "ğŸ“Š Access Points:"
echo "   Airflow UI:  http://localhost:8080"
echo "   Username:    airflow"
echo "   Password:    airflow123"
echo ""
echo "   PostgreSQL:  localhost:5433"
echo "   Username:    dataeng"
echo "   Password:    dataeng123"
echo "   Database:    news_warehouse"
echo ""
echo "ğŸ“ Next Steps:"
echo "   1. Open Airflow UI: http://localhost:8080"
echo "   2. Enable the 'news_pipeline_dag'"
echo "   3. Trigger a manual run or wait for scheduled execution"
echo ""
echo "ğŸ› ï¸  Useful Commands:"
echo "   View logs:        docker-compose logs -f"
echo "   Stop pipeline:    docker-compose down"
echo "   Restart:          docker-compose restart"
echo ""
echo "ğŸ“– For detailed instructions, see README.md" 